---
title: "Analyse d’un jeu de données"
author: "Guillaume LA & Samy OULMANE"
date: "2018/2019"
output:
  html_document:
    df_print: kable
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
subtitle: "Projet du 2e semestre - DFGSM3 - UE11 parcours d'informatique biomédicale"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", fig.width = 10)

# Libs
library(plyr)
library(readr)
library(tidyverse)
library(DT)
library(desctable)
library(cowplot)
library(corrplot)
library(RColorBrewer)
library(NbClust)
library(factoextra)
library(dendextend)
library(rpart)
library(rpart.plot)
library(class)
library(randomForest)
library(mclust)
library(caret)
# Importation des données ####
wdbc <- read_csv("data/wdbc.data", col_names = FALSE)

# Nom des colonnes
colnames <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity",  "concave_points", "symmetry",  "fractal_dimension")
variables <- gsub("_", " ", colnames) %>% str_to_title
colnames_mean <- paste0(colnames, "_mean")
colnames_se <- paste0(colnames, "_se")
colnames_worst <- paste0(colnames, "_worst")
colnames(wdbc) <- c("id", "diagnosis", colnames_mean, colnames_se, colnames_worst)

## Diagnosis en numérique (B=0, M=1), sans id
wdbc2 <-wdbc[-1]
wdbc2$diagnosis <- revalue(wdbc$diagnosis, c("B"=0, "M"=1)) %>% as.numeric

```

***

# Analyse descriptive

## Description de la population

Le jeu de données est constitué de `r nrow(wdbc)` entrées. Chacune de ces entrées correspond à une personne atteinte d'un cancer du sein chez qui une biopsie de la tumeur a été effectuée. Les noyaux des cellules tumorales on ensuite été analysé et les données recueillies sont présentées dans le tableau ci-dessous. Pour chaque variable, on a **la moyenne** (mean), **l'erreur standard** (SE) et **la "pire"" valeur** (worst, c'est-à-dire la moyenne des trois plus grandes valeurs de la variable).

```{r table_desc_1}

sketch = htmltools::withTags(table(
  class = 'display',
  thead(
    tr(
      th(rowspan = 2, 'ID'),
      th(rowspan = 2, 'Diagnosis'),
      th(colspan = 3, 'Radius'),
      th(colspan = 3, 'Texture'),
      th(colspan = 3, 'Perimeter'),
      th(colspan = 3, 'Area'),
      th(colspan = 3, 'Smoothness'),
      th(colspan = 3, 'Compactness'),
      th(colspan = 3, 'Concavity'),
      th(colspan = 3, 'Concave points'),
      th(colspan = 3, 'Symmetry'),
      th(colspan = 3, 'Fractal dimension')
    ),
    tr(
      lapply(rep(c('Mean', 'SE', 'Worst'), 10), th)
    )
  )
))

df <- data.frame(wdbc$id, wdbc$diagnosis,
           wdbc[,3], wdbc[,13], wdbc[,23],
           wdbc[,4], wdbc[,14], wdbc[,24],
           wdbc[,5], wdbc[,15], wdbc[,25],
           wdbc[,6], wdbc[,16], wdbc[,26],
           wdbc[,7], wdbc[,17], wdbc[,27],
           wdbc[,8], wdbc[,18], wdbc[,28],
           wdbc[,9], wdbc[,19], wdbc[,29],
           wdbc[,10], wdbc[,20], wdbc[,30],
           wdbc[,11], wdbc[,21], wdbc[,31],
           wdbc[,12], wdbc[,22], wdbc[,32])

DT::datatable(df, container = sketch, rownames = FALSE, options = list(scrollY = '350px', scrollX = 'true', paging = FALSE, dom = 'tp'),
              caption = 'Tableau 1 : jeu de données')

```

Les 10 variables mesurées sur les noyaux des cellules sont :

1. **Radius** : le rayon du noyau, qui est la moyenne des distances entre le centre du noyau et des points sur le périmètre
2. **Texture** : écart-type des valeurs des échelles de gris
3. **Perimeter** : le périmètre du noyau
6. **Area** : l'aire du noyau
7. **Smoothness** : la variation locale des longueurs du rayon
8. **Compactness**, compacité : perimeter^2 / area - 1
9. **Concavity**, concavité : sévérité des portions concaves du contour
10. **Concave points** : nombre de portions concaves du contour
11. **Symmetry** : la symétrie du noyau
12. **Fractal dimension** : "coastline approximation" - 1

Chaque échantillon a un identifiant et un diagnostic associé à la tumeur (maligne, M ; ou bénin, B). Il y a `r (wdbc$diagnosis %>% table)["B"]` tumeurs bénignes (`r round((wdbc$diagnosis %>% table)["B"]*100/nrow(wdbc),2)` %) et `r (wdbc$diagnosis %>% table)["M"]` (`r round((wdbc$diagnosis %>% table)["M"]*100/nrow(wdbc),2)` %) tumeurs malignes.

Le tableau suivant résume quelques paramètres statistiques pour chaque variables :

```{r table_desc_2}
desctable(wdbc[,-1], stats = list("N"       = length,
                             "%/Mean"  = is.factor ~ percent | (is.normal ~ mean),
                             "sd"      = is.normal ~ sd,
                             "Minimum" = min,
                             "Med"     = is.normal ~ NA | median,
                             "Maximum" = max,
                             "IQR"     = is.normal ~ NA | IQR)) %>%
  datatable(options = list(dom = 't', paging = FALSE, scrollY = '350px', scrollX = 'true'),
            caption = 'Tableau 2 : paramètres statistiques pour chaque variable.')
```

### Densités de répartition

La densité de répartition des valeurs de chaque variables est rapportée dans les graphiques suivants.

```{r fct_draw_g, include=FALSE}
draw_g <- function(value = "radius_mean", var_fill = NA, facet_diag = T) {
  g <- ggplot(data = wdbc) +
    aes_string(x = value) +
    geom_density(adjust = 1) +
    theme_minimal() +
    theme(legend.position = 'none',
          axis.title.y = element_blank(), axis.text.y = element_blank())
  if (facet_diag == T) {
    g <- g + facet_wrap(vars(diagnosis))
  }
  if (!is.na(var_fill)) {
    g <- g + aes_string(fill = var_fill)
  }
  g
}
```

#### Malins et bénins confondus {.tabset}

On voit que pour certaines variables comme le périmètre ou le nombre de points concaves, on a une distribution bimodale.

##### Moyenne {-}

```{r}
lapply(colnames_mean, draw_g, facet_diag = FALSE) %>% plot_grid(plotlist = .)
```

##### SE {-}

```{r}
lapply(colnames_se, draw_g, facet_diag = FALSE) %>% plot_grid(plotlist = .)
```

##### Worst {-}

```{r}
lapply(colnames_worst, draw_g, facet_diag = FALSE) %>% plot_grid(plotlist = .)
```


#### En fonction du diagnostic {.tabset}

##### Moyenne {-}

```{r var_graph_mean, fig.cap='Graphique 1 : répartition des moyennes pour chaque variable'}
lapply(colnames_mean, draw_g, var_fill = "diagnosis") %>% plot_grid(plotlist = .)
```

##### SE {-}

```{r var_graph_se, fig.cap='Graphique 2 : répartition des erreurs standard pour chaque variable'}
lapply(colnames_se, draw_g, var_fill = "diagnosis") %>% plot_grid(plotlist = .)
```

##### Worst {-}

```{r var_graph_worst, fig.cap='Graphique 3 : répartition des pires valeurs pour chaque variable'}
lapply(colnames_worst, draw_g, var_fill = "diagnosis") %>% plot_grid(plotlist = .)
```

### Corrélations {.tabset}

Pour savoir quelles sont les variables corrélées entre elles, on trace un diagrammes de corrélation  :

#### Moyennes {-}

```{r correlation_plot_mean}
wdbc2_mean <- wdbc2[,1:11]
cor(wdbc2_mean) %>%
  corrplot(type="upper", 
           method="number", 
           diag=FALSE,
           addCoefasPercent=TRUE,
           cl.pos=FALSE)

```

#### SE {-}

```{r correlation_plot_se}
wdbc2_se <- wdbc2[, c(1, 12:21)]
cor(wdbc2_se) %>% 
  corrplot(type="upper",
           method="number",
           diag=FALSE,
           addCoefasPercent=TRUE,
           cl.pos=FALSE)
```

#### Worst {-}

```{r correlation_plot_worst}
wdbc2_worst <- wdbc2[, c(1,22:31)]
cor(wdbc2_worst) %>%
  corrplot(type="upper",
           method="number",
           diag=FALSE,
           addCoefasPercent=TRUE,
           cl.pos=FALSE)
```

## Description des variables

### Diagnostic

C'est une variable catégorielle binaire qui renseigne sur le diagnostic associé à l'échantillon. Elle prend soit la valeur "B" pour désigner une tumeur bénigne, soit la valeur "M" pour une tumeur maligne.

```{r graph_diagnosis}
ggplot(data = wdbc) +
  aes(x = diagnosis, fill = diagnosis) +
  geom_bar(width = 0.5) +
  labs(title = 'Nombre de tumeurs en fonction du diagnostic',
    x = 'Diagnostic', y = 'Nombre') +
  annotate("text", c(1, 2), c(375, 230), label = c("357 (62,74 %)", "212 (37,26 %)")) +
  theme_minimal() +
  theme(legend.position = 'none')
```

Ainsi, on voit que deux tiers des tumeurs sont bénignes.

### Taille du noyau {.tabset}

Trois variables numériques continues renseignent sur la **taille** du noyau des cellules : le rayon (radius), le périmètre (perimeter) et l'aire (area). Elles sont corrélées entre elles et liées par des relations mathématiques.

```{r functions_desc_var, include=FALSE}
# Construit un graphique pour représenter une variable (boxplot + nuage de point) en fonction de la catégorie diagnostique
show.var <- function(variable, titre = "", abscisses = "", limites = NA) {
  ggplot(data = wdbc) +
    aes_string(x = "diagnosis", y = variable, fill = "diagnosis") +
    geom_boxplot() +
    geom_jitter(width = 0.1, alpha = 0.4) +
    theme_minimal() +
    theme(legend.position = 'none') +
    scale_y_continuous(limits = limites) +
    labs(x = "Diagnostic", title = titre, y = abscisses) +
    coord_flip()
}

# Construit une grille avec, pour chaque variable 3 graphiques : moyenne, erreur standard et worst.
# Les limites sont les mêmes, automatiquement calculées et alignées pour les graphiques de la moyenne et de worst pour permettre des les comparer
graphs.grid <- function(variable, variable.title, m = F) {
  # Calcul des limites pour les graphiques de moyenne et de pire
  limites = c(min(wdbc[,paste0(variable, "_mean")])*0.8, max(wdbc[,paste0(variable, "_worst")])*1.1)
  # Calcul des limites pour le graphique de l'erreur standard
  limites_se = c(0, max(wdbc[,paste0(variable, "_se")]))
  # Grille
  plot_grid(align = "hv",
    show.var(paste0(variable, "_mean"),
             str_to_sentence(paste0(variable.title, ifelse(m == T, " moyen", " moyenne"),' des noyaux')),
             str_to_sentence(paste0(variable.title, ifelse(m == T, " moyen", " moyenne"))),
             limites),
    show.var(paste0(variable, "_se"),
             paste0('Erreur standard ',
ifelse(m == T, "du ", ifelse(variable.title == "aire", "de l'", "de la ")), variable.title, ifelse(m == T, " moyen ", " moyenne "),'
des noyaux'),
             'Erreur standard',
             limites_se),
    show.var(paste0(variable, "_worst"),
             paste0('Pire ', variable.title,' des noyaux'),
             paste0('Pire ', variable.title),
             limites))
}
```

#### Rayon {-}

```{r radius_graphs, warning=FALSE, message=FALSE}
graphs.grid("radius", "rayon", T)
```

#### Périmètre {-}

```{r perimeter_graphs, warning=FALSE, message=FALSE}
graphs.grid("perimeter", "périmètre", T)
```

#### Aire {-}

```{r area_graphs, warning=FALSE, message=FALSE}
graphs.grid("area", "aire")
```

### Aspect {.tabset}

L'aspect du noyau est rapporté par deux variables numériques continues : la compacité et la texture. Comme pour la taille du  noyau, les valeurs pour les tumeurs malignes sont plus élevées que pour les bénignes.

#### Compactness {-}

La compacité est calculée par la formule suivante : $\frac{perimeter^2}{area} - 1$

```{r compactness_graphs, warning=FALSE, message=FALSE}
graphs.grid("compactness", "compacité")
```

#### Texture {-}

La texture du noyau est calculée à partie de l’écart-type des valeurs des échelles de gris.

```{r texture_graphs, warning=FALSE, message=FALSE}
graphs.grid("texture", "texture")
```

### Forme du noyau {.tabset}

Cinq variables numériques continues ont été mesurées pour rendre compte de la forme du noyau. On voit que la différence entre les cellules malignes et bénignes est la plus marquée pour les variables `concavity` et `concave points`. Les cellules tumorales malignes ont donc un noyau avec des contours plus concaves par rapport aux cellules bénignes. La dimension fractale par contre n'est pas différente entre les deux types de cellules.

#### Smoothness {-}

```{r smoothness_graphs, warning=FALSE, message=FALSE}
graphs.grid("smoothness", "régularité")
```

#### Concavity {-}

```{r concavity_graphs, warning=FALSE, message=FALSE}
graphs.grid("concavity", "concavité")
```

#### Concave points {-}

```{r concave_points_graphs, warning=FALSE, message=FALSE}
graphs.grid("concave_points", "nombre de points concaves", T)
```

#### Symmetry {-}

```{r symmetry_graphs, warning=FALSE, message=FALSE}
graphs.grid("symmetry", "symétrie")
```

#### Fractal dimension {-}

La dimension fractale est une variable numérique continue. Elle renseigne sur l'irrégularité des contours du noyau des cellules.

```{r fractal_dimension_graphs, warning=FALSE, message=FALSE}
graphs.grid("fractal_dimension", "dimension fractale")
```

## Variables pertinentes

La figure ci dessous montre les coefficients de corrélation entre le diagnostic et les autres variables.

```{r cor_diag}
cor_diag <- cor(wdbc2[,1], wdbc2[,c(1:31)])

corrplot(cor_diag, method = "number", diag = F, cl.pos = F, addCoefasPercent = T, tl.col = 'black')

cor_diag %>% t() %>% as.data.frame %>%
  rownames_to_column %>%
  dplyr::filter(diagnosis>0.5) %>%
  arrange(desc(diagnosis)) %>%
  select(rowname) %>%
  unlist %>% as.vector -> var_pert

var_pert <- var_pert[c(-1, -5, -7, -8, -9, -14, -16)]
```

Pour sélectionner les variables pertinentes, nous avons choisi de ne garder que celles qui sont corrélées à plus de 50% avec le diagnostic. Comme le périmètre, l'aire et le rayon sont liés entre eux, nous n'avons gardé que le périmètre, qui est la variable la plus corrélée des trois avec le diagnostic. On obtient alors `r length(var_pert)` variables pertinentes qui sont, de la plus corrélée à la moins corrélée : `r var_pert`.

Le graphique ci dessous montre le nuage de point obtenu quand on croise les deux variables les plus corrélées :

```{r graph_cor_diag}
ggplot(data = wdbc) +
  aes_string(x = var_pert[1], y = var_pert[2], color = "diagnosis") +
  geom_point() +
  labs(title = 'Variables les plus corrélées au diagnostic') +
  scale_color_discrete(name = "Diagnostic") +
  theme_minimal()
```

# Clustering 
 
La deuxième étape de notre analyse consistera à utiliser des algorithmes de clustering pour identifier des groupes au sein des données.    
Nous utiliserons trois algorithmes de clustering :

- k-means clustering
- clustering hiérarchique
- GMM-EM 

Nous avons vu lors de [l'étape de description du jeu de données](#111_densités_de_répartition) qu'une répartition bimodale pouvait apparaître avec certaines variables, suggérant l’existence de deux groupes. Nous verrons donc lors de cette étape de clustering si les algorithmes utilisés arriveront à constituer deux groupes distincts et homogènes.

Avant de pouvoir utiliser les algorithmes de clustering, on doit d'abord standardiser les données à l'aide de la fonction `scale`.

```{r scaling}
# Application de scale, en enlevant diagnosis
wdbc_scaled <- wdbc2[-1] %>% apply(2, FUN=scale) %>% as.data.frame
```

## K-means

Le premier algorithme utilise la méthode des **k-moyennes**.

```{r model_kmeans}
# Construction du modèle en imposant deux clusters
model_km <- kmeans(wdbc_scaled, 2)

# Visualisation des clusters
fviz_cluster(data = wdbc_scaled, model_km)

# Taille des clusters
questionr::freq(model_km$cluster, valid = F)

km_perf <- function (model) {
  attach(model)
  df <- data.frame(paste0("Cluster 1 : ", round(withinss[1], 2), "<br>Cluster 2 : ", round(withinss[2], 2)), round(totss, 2), round(tot.withinss, 2), round(betweenss, 2))
  return(knitr::kable(df,col.names = c("Somme intra-cluster", "Somme totale", "Somme totale intra-cluster", "Somme inter-cluster"), align = "l"))
  detach(model)
}
```

Le tableau suivant résume les performances de l'algorithme en utilisant les sommes inter-clusters et intra-clusters *des carrés* des distances entre les données.

`r km_perf(model_km)`

## Clustering hiérarchique

Les résultats d'un clustering hiérarchique sont visualisables avec un **dendrogramme**.

```{r model_hclustering}
# Construction du modèle
model_hc <- dist(wdbc_scaled) %>% hclust(method = "ward.D") %>% as.dendrogram()

# On coupe le denrogramme de sorte à n'avoir que deux clusters
model_hc_cluster <- model_hc %>% cutree(2)

# Visualisation
fviz_dend(model_hc, 2)

# Taille des clusters
questionr::freq(model_hc_cluster, valid = F)
```

## Mélange gaussien avec espérance-maximisation

Le troisième algorithme de clustering que nous avons choisi est l'algorithme d'espérance-maximisation sur un modèle de mélange gaussien. C'est un algorithme de soft-clustering donc chaque individu a une **probabilité** d’appartenir à l'un ou l'autre des clusters, ce qui est particulièrement approprié dans le contexte d'une décision médicale lorsqu'il s'agit de poser un diagnostic.

=> Les deux autres algorithmes utilisés précédemment sont des algorithmes de hard-clustering c'est-à-dire que chaque individu est assigné à un cluster de façon binaire. On ne sait pas avec quelle certitude l'assignation est faite.

```{r model_gmm_em}
# Construction du modèle avec le jeu de données réduit (meilleure )
model_em <- Mclust(wdbc_scaled, G=2)

# Visualisationd es clusters
fviz_mclust(model_em)

# Taille des clusters
questionr::freq(model_em$classification, valid = F)

table(wdbc$diagnosis, model_em$classification)
```

## Evaluation des modèles

En résumé, les trois algorithmes de clustering identifient bien les deux clusters.

Pour évaluer la cohérence des clusters, nous avons utilisé le coefficient de **silhouette** et sa représentation graphique. La ligne rouge en pointillé représente le score moyen.

```{r}
# Calcul des coefficients de silhouette pour chaque modèle
sil_km <- cluster::silhouette(model_km$cluster, dist(wdbc_scaled))
sil_hc <- cluster::silhouette(model_hc_cluster, dist(wdbc_scaled))
sil_em <- cluster::silhouette(model_em$classification, dist(wdbc_scaled))

# Visualisation graphique
plot_grid(fviz_silhouette(sil_km, subtitle = "KM"),
          fviz_silhouette(sil_hc, subtitle = "HC"),
          fviz_silhouette(sil_em, subtitle = "EM"))
```

On remarque qu'à chaque fois, c'est le plus grand des clusters qui obtient les scores silhouette les plus élevés et qu'ils sont quasiment tous positifs. En revanche, le cluster le plus petit est moins cohérent, avec des scores qui sont presque tous en dessous de la moyenne pour les trois algorithmes.

# Classification

En dernière partie de notre analyse, on utilisera des algorithmes de classification pour prédire la classe diagnostique. Mais pour appliquer ces algorithmes, on doit d'abord diviser le jeu de données en deux parties :

- une partie entrainement (train) sur laquelle se fera l'apprentissage
- et une partie vérification (test) sur laquelle on appliquera l'algorithme de classification pour prédique la classe diagnostique.

Les trois algorithmes utilisé sont :

- l'arbre de décision 
- la régression logistique
- random forest

```{r split_data, include=FALSE}
set.seed(42)

# TEST SET
# On garde 20% des données pour le test
ntest <- sample(1:nrow(wdbc2), 0.2*nrow(wdbc2), replace=FALSE)
# On s'assure du maintient des proportions de chaque catégorie diagnostique entre les sous-échantillon
wdbc2_test1 <- wdbc2[wdbc2$diagnosis=="1",] %>% sample_n(round(0.2*sum(wdbc2$diagnosis == 1)))
wdbc2_test0 <- wdbc2[wdbc2$diagnosis=="0",] %>% sample_n(round(0.2*sum(wdbc2$diagnosis == 0)))
wdbc2_test <- bind_rows(wdbc2_test1, wdbc2_test0)
wdbc2_test$diagnosis <- wdbc2_test$diagnosis %>% as.factor

# TRAIN SET
wdbc2_train <- anti_join(wdbc2, wdbc2_test)
wdbc2_train$diagnosis <- wdbc2_train$diagnosis %>% as.factor
```

## Arbre de décisions

Le premier algorithme de classification est **l'arbre de décision**. Cet algorithme a l'avantage d'être intuitif et d'expliciter les variables utilisées pour classifier les données.

```{r decision_tree}
model_dt <- rpart(diagnosis ~ ., data=wdbc2_train)

# Visualisation
rpart.plot(model_dt)

# Calcul des performances avec caret, qui permet de générer une matrice de confusion        
perf_dt <- predict(model_dt, newdata=wdbc2_test ,type="class") %>% 
  table(wdbc2_test$diagnosis) %>%
  confusionMatrix
```

On retrouve ici les variables `perimeter_worst` ou `radius_worst` et `concave_points_worst` comme critères de décision, ce qui est cohérent avec ce qu'on avait trouvé [plus haut](#13_variables_pertinentes) lors de la recherche des variables pertinentes. En effet, ces deux variables sont celles qui sont les plus corrélées au diagnostic.

Néanmoins cette relation n'a pas été observée avec `texture_worst`, dont le coefficient de corrélation avec `diagnosis` n'est que le 46 %.

## Régression logistique

Etant donné que nous devons prédire une variable binaire dans le cadre d'une analyse multivariée, nous avons choisi d'utiliser un algorithme de régression logistique.

```{r log_reg, message=FALSE, warning=FALSE}
# Génération du modèle
model_lr <- glm(diagnosis ~., wdbc2_train, family=binomial)

# Calcul des performances avec caret, qui permet de générer une matrice de confusion
perf_lr <- model_lr %>%
              predict(wdbc2_test, type="response") %>% 
              round() %>% 
              table(wdbc2_test$diagnosis) %>%
              confusionMatrix
```

## Random forest

L'algorithme de random forest est la méthode ensembliste que nous avons choisi.

```{r random_forest}
# Génération du modèle
model_rf <- randomForest(diagnosis ~ ., wdbc2_train, prox=TRUE)

# Calcul des performances avec caret, qui permet de générer une matrice de confusion
perf_rf <- model_rf %>%
              predict(newdata=wdbc2_test, type="response") %>%
              table(wdbc2_test$diagnosis) %>%
              confusionMatrix
```

## Comparaison des modèles

Pour comparer les performances des différents algorithmes, on avons utilisé le package `caret` pour produire des matrices de confusion et différents indicateurs de qualité des modèles qui sont la **sensibilité**, la **specificité** et les **valeurs prédictives positives** et **négative**.

```{r models_comparing, warning=FALSE}
comparing <- data.frame(round(perf_dt$byClass[1:4]*100, 2), 
                        round(perf_lr$byClass[1:4]*100, 2), 
                        round(perf_rf$byClass[1:4]*100, 2)) %>% t

rownames(comparing) <- c("KNN", "Régression logistique", "Random forest")
colnames(comparing) <- c("Sensibilité", "Specificité", "VPP", "VPN")

DT::datatable(comparing, options = list(dom = 't'))
```

Après plusieurs essais, random forest est l'algorithme qui obtient les meilleures performances, alors que la régression logistique obtient les pires.


