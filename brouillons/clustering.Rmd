---
title: "Analyse d’un jeu de données"
author: "Guillaume LA & Samy OULMANE"
date: "2018/2019"
output:
  html_document:
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
subtitle: "Projet du 2e semestre - DFGSM3 - UE11 parcours d'informatique biomédicale"
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", fig.width = 10)

# Libs
library(plyr)
library(readr)
library(tidyverse)
library(DT)
library(desctable)
library(cowplot)
library(corrplot)
library(RColorBrewer)
library(NbClust)
library(factoextra)
library(dendextend)

# Importation des données ####
wdbc <- read_csv("../data/wdbc.data", col_names = FALSE)

# Nom des colonnes
colnames <- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity",  "concave_points", "symmetry",  "fractal_dimension")
variables <- gsub("_", " ", colnames) %>% str_to_title
colnames_mean <- paste0(colnames, "_mean")
colnames_se <- paste0(colnames, "_se")
colnames_worst <- paste0(colnames, "_worst")
colnames(wdbc) <- c("id", "diagnosis", colnames_mean, colnames_se, colnames_worst)

## Diagnosis en numérique (B=1, M=2)
wdbc2 <- wdbc
wdbc2$diagnosis <- revalue(wdbc$diagnosis, c("B"="1", "M"="2")) %>% as.numeric
```

***

# Clustering

La deuxième étape de notre analyse consistera à utiliser des algorithmes de clustering pour identifier des groupes au sein des données.    
Nous utiliserons trois algorithmes de clustering :

- k-means clustering
- clustering hiérarchique
- EM 

La démarche sera la même à chaque fois : on commencera d'abord avec les données "brutes" c'est-à-dire en utilisant *toutes* les variables et ensuite en utilisant seulement quelques variables sélectionnées. Notre but étant de pouvoir consituer des clusters proches des catégories diagnostiques (bénin/malin).

Avant de pouvoir utiliser les algorithmes de clustering, on doit d'abord standardiser les données à l'aide de la fonction `scale`.

```{r scaling}
# Données sans id ni diagnosis
wdbc_blind <- wdbc[c(-1, -2)]
# Application de scale
wdbc_scaled <- wdbc_blind %>% apply(2, FUN=scale) %>% as.data.frame
```

## K-means

```{r}
# Construction du modèle en imposant deux clusters
model_km <- kmeans(wdbc_scaled, 2)

# Visualisation des clusters
fviz_cluster(data = wdbc_scaled, model_km)
```

L'algorithme produit deux clusters de tailles `r model_km$size[1]` et `r model_km$size[2]`.

```{r fun_km_perf}
km_perf <- function (model) {
  attach(model)
  df <- data.frame(paste0("Cluster 1 : ", round(withinss[1], 2), "<br>Cluster 2 : ", round(withinss[2], 2)),
                   round(totss, 2),
                   round(tot.withinss, 2),
                   round(betweenss, 2))
  return(knitr::kable(df,col.names = c("Somme intra-cluster", "Somme totale", "Somme totale intra-cluster", "Somme inter-cluster"), align = "l"))
  detach(model)
}
```

Le tableau suivant résume les performances de l'algorithme en utilisant les sommes inter-clusters et intra-clusters *des carrés* des distances entre les données.

`r km_perf(model_km)`


